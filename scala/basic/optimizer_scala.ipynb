{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "In gradient-base optimization algorithms, we update the parameters (or weights) using the gradients in each iteration. We call this updating function as Optimizer.\n",
    "\n",
    "The main method of an optimizer is update(weight, grad), which updates a NDArray weight using a NDArray gradient. But given that a multi-layer neural network often has more than one weights, we assign each weight a unique integer index. Furthermore, an optimizer may need space to store auxiliary state, such as momentum, we also allow a user-defined state for updating. In summary, an optimizer has two major methods\n",
    "\n",
    "- create_state(index, weight): create auxiliary state for the index-th weight.\n",
    "- update(index, weight, grad, state): update the index-th weight given the gradient and auxiliary state. The state can be also updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Scala kernel\n",
    "Add mxnet scala jar which is created as a part of MXNet Scala package installation in classpath as follows:\n",
    "\n",
    "**Note**: Process to add this jar in your scala kernel classpath can differ according to the scala kernel you are using.\n",
    "\n",
    "We have used [jupyter-scala kernel](https://github.com/alexarchambault/jupyter-scala) for creating this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "classpath.addPath(<path_to_jar>)\n",
    "\n",
    "e.g\n",
    "classpath.addPath(\"mxnet-full_2.11-osx-x86_64-cpu-0.1.2-SNAPSHOT.jar\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "### Create and Update\n",
    "MXNet has already implemented several popular optimizers in python/mxnet/optimizer.py. An convenient way to create one is by using optimizer.create(name, args...). The following codes create a standard SGD updater which does\n",
    "\n",
    "```scala\n",
    "weight = weight - learning_rate * grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mml.dmlc.mxnet.optimizer.SGD\u001b[0m\n",
       "\u001b[36mopt\u001b[0m: \u001b[32mml\u001b[0m.\u001b[32mdmlc\u001b[0m.\u001b[32mmxnet\u001b[0m.\u001b[32moptimizer\u001b[0m.\u001b[32mSGD\u001b[0m = ml.dmlc.mxnet.optimizer.SGD@18d0acf8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ml.dmlc.mxnet._\n",
    "import ml.dmlc.mxnet.optimizer.SGD\n",
    "//val opt = new Optimizer(\"sgd\", learningRate=0.1f)\n",
    "val opt = new SGD(learningRate=0.1f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the update function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgrad\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@96420bdf\n",
       "\u001b[36mweight\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@f4b4d3bf\n",
       "\u001b[36mindex\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m0\u001b[0m\n",
       "\u001b[36mres9_4\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mFloat\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m, \u001b[32m0.89999F\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val grad = NDArray.ones(2,3)\n",
    "val weight = NDArray.ones(2,3)\n",
    "val index = 0\n",
    "opt.update(index, weight, grad, NDArray.empty(2,3))\n",
    "weight.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When momentum is non-zero, the sgd optimizer needs extra state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmomOpt\u001b[0m: \u001b[32mSGD\u001b[0m = ml.dmlc.mxnet.optimizer.SGD@17af5997\n",
       "\u001b[36mindex\u001b[0m: \u001b[32mInt\u001b[0m = \u001b[32m0\u001b[0m\n",
       "\u001b[36mgrad\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@f7a94dad\n",
       "\u001b[36mweight\u001b[0m: \u001b[32mNDArray\u001b[0m = ml.dmlc.mxnet.NDArray@c02afa4c\n",
       "\u001b[36mstate\u001b[0m: \u001b[32mAnyRef\u001b[0m = ml.dmlc.mxnet.NDArray@e31b5efa\n",
       "\u001b[36mres11_6\u001b[0m: \u001b[32mAnyRef\u001b[0m = ml.dmlc.mxnet.NDArray@bf9df5e6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val momOpt = new SGD(learningRate = 0.1f, momentum = 0.01f)\n",
    "val index = 0\n",
    "val grad = NDArray.ones(2,3)\n",
    "val weight = NDArray.ones(2,3)\n",
    "val state = momOpt.createState(index, weight)\n",
    "opt.update(index, weight, grad, state)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Reading\n",
    "[Optimizer API](http://mxnet.io/api/scala/docs/index.html#ml.dmlc.mxnet.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
